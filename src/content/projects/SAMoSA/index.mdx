---
type: 'project'
slug: SAMoSA
title: SAMoSA
subtitle: Sensing Activities with Motion and Subsampled Audio
authors:
  - Vimal Mollyn
  - Karan Ahuja
  - Dhruv Verma
  - Chris Harrison
  - Mayank Goel

year: 2022
coverImage: './images/cover.png'
published: 'yes'
ishomePage: 'yes'
award:
pdfLink: '/papers/samosa.pdf'
github: https://github.com/cmusmashlab/SAMoSA
videoLink: https://www.youtube.com/embed/QEJBaD71x_s?si=auK0Zio-IBCiWrnW
conference: "UbiComp '22: Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies"
conferencePage: 'https://doi.org/10.1145/3550284'
citation: 'Mollyn, V., Ahuja, K., Verma, D., Harrison, C., & Goel, M. (2022). SAMoSA: Sensing activities with motion and subsampled audio. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 6(3), 1-19.'
bibtex: |
  @article{mollyn2022samosa,
    title={SAMoSA: Sensing activities with motion and subsampled audio},
    author={Mollyn, Vimal and Ahuja, Karan and Verma, Dhruv and Harrison, Chris and Goel, Mayank},
    journal={Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
    volume={6},
    number={3},
    pages={1--19},
    year={2022},
    publisher={ACM New York, NY, USA}
  }
---

## Abstract

Despite advances in audio- and motion-based human activity recognition (HAR) systems, a practical, power-efficient, and privacy-sensitive activity recognition system has remained elusive. State-of-the-art activity recognition systems often require power-hungry and privacy-invasive audio data. This is especially challenging for resource-constrained wearables, such as smartwatches. To counter the need for an always-on audio-based activity classification system, we first make use of power and compute-optimized IMUs sampled at 50 Hz to act as a trigger for detecting activity events. Once detected, we use a multimodal deep learning model that augments the motion data with audio data captured on a smartwatch. We subsample this audio to rates â‰¤ 1 kHz, rendering spoken content unintelligible, while also reducing power consumption on mobile devices. Our multimodal deep learning model achieves a recognition accuracy of 92.2% across 26 daily activities in four indoor environments. Our findings show that subsampling audio from 16 kHz down to 1 kHz, in concert with motion data, does not result in a significant drop in inference accuracy. We also analyze the speech content intelligibility and power requirements of audio sampled at less than 1 kHz and demonstrate that our proposed approach can improve the practicality of human activity recognition systems.
