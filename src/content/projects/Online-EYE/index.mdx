---
type: 'project'
slug: Online-EYE
title: Online-EYE
subtitle: Multimodal Implicit Eye Tracking Calibration for XR
authors:
  - Baosheng James Hou
  - Lucy Abramyan
  - Prasanthi Gurumurthy
  - Haley Adams
  - Ivana Tosic Rodgers
  - Eric J Gonzalez
  - Khushman Patel
  - Andrea Cola√ßo
  - Ken Pfeuffer
  - Hans Gellersen
  - Karan Ahuja
  - Mar Gonzalez-Franco

year: 2025
dateAdded: '2025-05-07'
coverImage: './images/cover.png'
published: 'yes'
ishomePage: 'no'
award:
pdfLink: '/papers/Online-EYE.pdf'
github: ''
videoLink: 'https://www.youtube.com/embed/4GvfR5G2rps?si=6S3evCPOvWl7iNZw'
conference: "CHI '25: Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems"
conferencePage: 'https://dl.acm.org/doi/10.1145/3706598.3713461'
citation: 'Hou, B. J., Abramyan, L., Gurumurthy, P., Adams, H., Tosic Rodgers, I., Gonzalez, E. J., ... & Gonzalez-Franco, M. (2025, April). Online-EYE: Multimodal Implicit Eye Tracking Calibration for XR. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems (pp. 1-16).'
bibtex: |
  @inproceedings{hou2025online,
    title={Online-EYE: Multimodal Implicit Eye Tracking Calibration for XR},
    author={Hou, Baosheng James and Abramyan, Lucy and Gurumurthy, Prasanthi and Adams, Haley and Tosic Rodgers, Ivana and Gonzalez, Eric J and Patel, Khushman and Cola{\c{c}}o, Andrea and Pfeuffer, Ken and Gellersen, Hans and others},
    booktitle={Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
    pages={1--16},
    year={2025}
    }
---

## Abstract

Unlike other inputs for extended reality (XR) that work out of the box, eye tracking typically requires custom calibration per user or session. We present a multimodal inputs approach for implicit calibration of eye tracker in VR, leveraging UI interaction for continuous, background calibration. Our method analyzes gaze data alongside controller interaction with UI elements, and employing ML techniques it continuously refines the calibration matrix without interrupting users from their current tasks. Potentially eliminating the need for explicit calibration. We demonstrate the accuracy and effectiveness of this implicit approach across various tasks and real time applications achieving comparable eye tracking accuracy to native, explicit calibration. While our evaluation focuses on VR and controller-based interactions, we anticipate the broader applicability of this approach to various XR devices and input modalities.

